---
title: "第9章: RNN 与 CNN"
date: 2020-09-09 19:46:20 +0800
layout: single
toc: true
lang: zh
sidebar: {nav: "sidebar_zh"}
header:
  teaser: /assets/images/ch09.png
---

## 80. 将单词转换为数字ID 
考虑第51问中所创建的数据，试将其中的单词按如下规则转化为ID：
+ 按在数据集中的词频对这些单词降序排序，ID与词频的排名相对应（词频最高的单词ID为`1`，词频第二高的单词ID为`2`，以此类推）；
+  对于所有词频为1的单词，令它们的ID为`0`。

实现一个函数以满足上述要求，接收单词的序列作为输入，返回其对应ID的序列作为输出。

## 81. 使用RNN进行预测
现有一个ID的序列$$\boldsymbol{x} = (x_1, x_2, \dots, x_T)$$，与某个单词的序列相对应。 其中，$$T$$表示该序列中单词的总数，$$x_t \in \mathbb{R}^{V}$$表示第$$t$$个单词所对应的ID的one-hot编码向量（$$V$$表示词表中单词的总数）。 使用循环神经网络（RNN, Recurrent Neural Network）构建一个模型，按下式通过ID的序列$$\boldsymbol{x}$$预测其所属的类别$$y$$：

$$
\overrightarrow{h}_0 = 0, \\
\overrightarrow{h}_t = {\rm \overrightarrow{RNN}}(\mathrm{emb}(x_t), \overrightarrow{h}_{t-1}), \\
y = {\rm softmax}(W^{(yh)} \overrightarrow{h}_T + b^{(y)}),
$$

其中，$$\mathrm{emb}(x) \in \mathbb{R}^{d_w}$$表示词嵌入，是一个将one-hot编码向量（稀疏，高维）映射到词向量（密集，低维）的函数。$$\overrightarrow{h}_t \in \mathbb{R}^{d_h}$$表示时序上$$t$$时刻的隐状态（hidden state）。$${\rm \overrightarrow{RNN}}(x,h)$$表示一个RNN单元，利用当前时刻$$t$$的输入$$x$$与前一时刻的隐状态$$h$$计算下一个时刻的隐状态。
$$W^{(yh)} \in \mathbb{R}^{L \times d_h}$$ 是一个线性映射矩阵，从隐状态的空间映射到用于预测类别的空间，$$b^{(y)} \in \mathbb{R}^{L}$$是偏置项（$$d_w$$, $$d_h$$ 和 $$L$$分别表示词向量的维度，隐状态的维度与类别数量）。RNN单元$${\rm \overrightarrow{RNN}}(x,h)$$可以有各种不同的构成，如：

$$
{\rm \overrightarrow{RNN}}(x,h) = g(W^{(hx)} x + W^{(hh)}h + b^{(h)}),
$$

其中，$$W^{(hx)} \in \mathbb{R}^{d_h \times d_w}，W^{(hh)} \in \mathbb{R}^{d_h \times d_h}, b^{(h)} \in \mathbb{R}^{d_h}$$是RNN单元的参数，$$g$$表示激活函数（如$$\tanh$$，ReLU等）。

这里暂不需要训练模型，只需随机初始化模型参数并计算分类$$y$$即可。合理设置参数的维度，如$$d_w = 300, d_h=50$$。后续问题将沿用本问的设定。

## 82. 使用随机梯度下降法进行训练

使用随机梯度下降法（SGD，Stochastic Gradient Descent）训练第81问中的模型，训练过程中，打印出模型在训练集与验证集上的损失与正确率。选用适当的基准终止训练（如“10个Epochs后停止”）。

## 83. Mini-batch, GPU训练

修改第82问中的代码，使其以每$$B$$个实例为单位计算损失与梯度，并在GPU上进行训练（适当选择$$B$$的值）。

## 84. 使用预训练词向量

使用预训练词向量（如[Google News embeddings](https://drive.google.com/file/d/0B7XkCwpI5KDYNlNUTTlSS21pQmM/edit?usp=sharing)，在约1,000亿单词上训练所得）初始化词嵌入$$\mathrm{emb}(x)$$，并训练模型。

## 85. 双向RNN与多层RNN

使用前向和反向RNN对输入文本进行编码，并训练模型。

$$
\overleftarrow{h}_{T+1} = 0, \\
\overleftarrow{h}_t = {\rm \overleftarrow{RNN}}(\mathrm{emb}(x_t), \overleftarrow{h}_{t+1}), \\
y = {\rm softmax}(W^{(yh)} [\overrightarrow{h}_T; \overleftarrow{h}_1] + b^{(y)})
$$

其中，$$\overrightarrow{h}_t \in \mathbb{R}^{d_h}, \overleftarrow{h}_t \in \mathbb{R}^{d_h}$$分别代表时刻$$t$$时，在前向和反向RNN上求得的隐状态向量。
$${\rm \overleftarrow{RNN}}(x,h)$$使用当前时刻的输入$$x$$与下一时刻的隐状态$$h$$计算前一时刻的隐状态，$$W^{(yh)} \in \mathbb{R}^{L \times 2d_h}$$是一个线性映射矩阵，从隐状态的空间映射到用于预测类别的空间，$$b^{(y)} \in \mathbb{R}^{L}$$是偏置项。
$$[a; b]$$表示两个向量$$a$$与$$b$$的拼接（concatenation）。


在此基础上，增加双向RNN的层数，并重复此实验。

### 86. 卷积神经网络 (CNN)

现有一个ID的序列$$\boldsymbol{x} = (x_1, x_2, \dots, x_T)$$，与某个单词的序列相对应。 其中，$$T$$表示该序列中单词的总数，$$x_t \in \mathbb{R}^{V}$$表示第$$t$$个单词所对应的ID的one-hot编码向量（$$V$$表示词表中单词的总数）。使用卷积神经网络（CNN, Convolutional Neural Network）构建一个模型，通过ID的序列$$\boldsymbol{x}$$预测其所属的类别$$y$$。

CNN的设定如下： 

+ 词嵌入向量的维度: $$d_w$$
+ 卷积核（Filter）：3个词
+ 卷积步长（Stride）：1个词
+ 卷积填充（Padding）：启用
+ 每次卷积操作后产生的向量维度：$$d_h$$
+ 每次卷积操作后，使用最大池化（max-pooling）将输入句子表示为$$d_h$$维的向量

即，$$t$$时刻的特征向量$$p_t \in \mathbb{R}^{d_h}$$以下述公式计算：

$$
p_t = g(W^{(px)} [\mathrm{emb}(x_{t-1}); \mathrm{emb}(x_t); \mathrm{emb}(x_{t+1})] + b^{(p)})
$$

其中, $$W^{(px)} \in \mathbb{R}^{d_h \times 3d_w}$$, $$b^{(p)} \in \mathbb{R}^{d_h}$$代表CNN的参数, $$g$$是激活函数（如$$\tanh$$，ReLU等），$$[a; b; c]$$表示向量$$a,b,c$$的拼接。矩阵$$W^{(px)}$$的列数为$$3d_w$$，因为需要对3个拼接后的词向量进行线性映射。

最大池化操作中，取所有时刻的特征向量在各个维度上的最大值，获得输入文章的特征向量$$c \in \mathbb{R}^{d_h}$$。假设$$c[i]$$代表向量$$c$$的第$$i$$个维度，最大池化以下述公式计算：

$$
c[i] = \max_{1 \leq t \leq T} p_t[i]
$$

最后，使用矩阵$$W^{(yc)} \in \mathbb{R}^{L \times d_h}$$及偏置$$b^{(y)} \in \mathbb{R}^{L}$$对输入文章的特征向量进行线性映射，然后使用softmax函数预测其所属类别$$y$$：

$$
y = {\rm softmax}(W^{(yc)} c + b^{(y)})
$$

这里暂不需要训练模型，只需随即初始化模型参数并计算$$y$$即可。

## 87. 使用随机梯度下降法进行CNN训练

使用随机梯度下降法（SGD）训练第86问中的模型，训练过程中，打印出模型在训练集与验证集上的损失与正确率。选用适当的基准终止训练（如“10个Epochs后停止”）。

## 88. 超参数调节

修改第85问或第87问中的代码，改变神经网络的架构或调节超参数，以构建一个高性能的分类器。

## 89. 基于预训练语言模型的迁移学习

从预训练的语言模型（如[BERT](https://github.com/google-research/bert)等）出发，构建一个能对新闻标题进行分类的模型。


