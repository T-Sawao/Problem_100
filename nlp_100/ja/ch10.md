---
title: "第10章: 機械翻訳"
date: 2020-04-07 08:30:53 +0900
layout: single
toc: true
lang: ja
sidebar: {nav: "sidebar_ja"}
header:
  teaser: /assets/images/ch10.png
---

本章では，日本語と英語の翻訳コーパスである[京都フリー翻訳タスク (KFTT)](http://www.phontron.com/kftt/index-ja.html)を用い，ニューラル機械翻訳モデルを構築する．ニューラル機械翻訳モデルの構築には，[fairseq](https://github.com/pytorch/fairseq)，[Hugging Face Transformers](https://github.com/huggingface/transformers)，[OpenNMT-py](https://github.com/OpenNMT/OpenNMT-py)などの既存のツールを活用せよ．

## 90. データの準備

機械翻訳のデータセットをダウンロードせよ．訓練データ，開発データ，評価データを整形し，必要に応じてトークン化などの前処理を行うこと．ただし，この段階ではトークンの単位として形態素（日本語）および単語（英語）を採用せよ．

## 91. 機械翻訳モデルの訓練

90で準備したデータを用いて，ニューラル機械翻訳のモデルを学習せよ（ニューラルネットワークのモデルはTransformerやLSTMなど適当に選んでよい）．

## 92. 機械翻訳モデルの適用

91で学習したニューラル機械翻訳モデルを用い，与えられた（任意の）日本語の文を英語に翻訳するプログラムを実装せよ．

## 93. BLEUスコアの計測

91で学習したニューラル機械翻訳モデルの品質を調べるため，評価データにおけるBLEUスコアを測定せよ．

## 94. ビーム探索

91で学習したニューラル機械翻訳モデルで翻訳文をデコードする際に，ビーム探索を導入せよ．ビーム幅を1から100くらいまで適当に変化させながら，開発セット上のBLEUスコアの変化をプロットせよ．

## 95. サブワード化

トークンの単位を単語や形態素からサブワードに変更し，91-94の実験を再度実施せよ．

## 96. 学習過程の可視化

[Tensorboard](https://www.tensorflow.org/tensorboard)などのツールを用い，ニューラル機械翻訳モデルが学習されていく過程を可視化せよ．可視化する項目としては，学習データにおける損失関数の値とBLEUスコア，開発データにおける損失関数の値とBLEUスコアなどを採用せよ．

## 97. ハイパー・パラメータの調整

ニューラルネットワークのモデルや，そのハイパーパラメータを変更しつつ，開発データにおけるBLEUスコアが最大となるモデルとハイパーパラメータを求めよ．

## 98. ドメイン適応

[Japanese-English Subtitle Corpus (JESC)](https://nlp.stanford.edu/projects/jesc/index_ja.html)や[JParaCrawl](http://www.kecl.ntt.co.jp/icl/lirg/jparacrawl/)などの翻訳データを活用し，KFTTのテストデータの性能向上を試みよ．

## 99. 翻訳サーバの構築

ユーザが翻訳したい文を入力すると，その翻訳結果がウェブブラウザ上で表示されるデモシステムを構築せよ．
